{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97398a9",
   "metadata": {},
   "source": [
    "## This note book is currently in the works\n",
    "\n",
    "I want my model to be explainable. One of the best ways to do that is o generate attention for each generated caption. I want to be able to hover over any generated work and show a heatmap layed over the input image explaining why that word was generated. This is harder than I thought but we'll get there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc736974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "# from peft import PeftModel\n",
    "# from PIL import Image\n",
    "# import cv2\n",
    "\n",
    "\n",
    "# def get_attention_maps(model):\n",
    "#     attention_maps = []\n",
    "\n",
    "#     def hook_fn(module, input, output):\n",
    "#         # output: attention weights shape (batch_size, num_heads, tgt_len, src_len)\n",
    "#         attention_maps.append(output[1].detach().cpu())  # cross-attention weights\n",
    "\n",
    "#     # Access decoder cross-attention layers\n",
    "#     for layer in model.text_decoder.bert.encoder.layer:\n",
    "#         # Register on the cross-attention module\n",
    "#         layer.crossattention.register_forward_hook(hook_fn)\n",
    "\n",
    "#     return attention_maps\n",
    "\n",
    "# def visualize_attention(image, attention_map, word, patch_size=16):\n",
    "#     # attention_map: (1, 577) â†’ remove CLS\n",
    "#     attn = attention_map[0, 1:]  # Remove CLS token, now shape [576]\n",
    "#     print(attn)\n",
    "\n",
    "#     # Compute patch grid\n",
    "#     num_patches = attn.shape[0]\n",
    "#     h = w = int(np.sqrt(num_patches))\n",
    "#     if h * w != num_patches:\n",
    "#         raise ValueError(f\"Cannot reshape attention of size {num_patches} into square grid\")\n",
    "\n",
    "#     attn = attn.reshape(h, w).numpy()\n",
    "#     attn = (attn - attn.min()) / (attn.max() - attn.min())  # normalize to [0,1]\n",
    "\n",
    "#     # Resize attention to match image size\n",
    "#     attn_resized = cv2.resize(attn, image.size)  # use OpenCV to resize heatmap\n",
    "\n",
    "#     # Overlay heatmap on image\n",
    "#     plt.imshow(image)\n",
    "#     plt.imshow(attn_resized, cmap='jet', alpha=0.5)\n",
    "#     plt.title(f\"Attention for '{word}'\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def describe_image_with_attention(img):\n",
    "#     base_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "#     adapter_path = os.path.join('./blip-lora-finetuned/checkpoint-4770/')\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#     processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "#     base_model = BlipForConditionalGeneration.from_pretrained(base_model_name)\n",
    "#     model = PeftModel.from_pretrained(base_model, adapter_path).to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "#     pixel_values = inputs['pixel_values']\n",
    "\n",
    "#     # Capture attention maps\n",
    "#     attention_maps = get_attention_maps(model)\n",
    "\n",
    "#     # Generate caption\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             pixel_values=pixel_values,\n",
    "#             max_length=20,\n",
    "#             num_beams=5,\n",
    "#             early_stopping=True,\n",
    "#             output_attentions=True,\n",
    "#             return_dict_in_generate=True\n",
    "#         )\n",
    "\n",
    "#     caption_ids = outputs.sequences[0]\n",
    "#     caption = processor.decode(caption_ids, skip_special_tokens=True)\n",
    "#     print(\"Caption:\", caption)\n",
    "\n",
    "#     # Visualize attention for selected words\n",
    "#     words = caption.split()\n",
    "#     for i, word in enumerate(words):\n",
    "#         if i < len(attention_maps):  # Safety check\n",
    "#             visualize_attention(img, attention_maps[i][0], word)\n",
    "\n",
    "#     return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dece16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError(\"Connection broken: InvalidChunkLength(got length b'', 0 bytes read)\", InvalidChunkLength(got length b'', 0 bytes read)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a clear glass on a white background\n",
      "tensor([], size=(0, 577))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdescribe_image_with_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./glass.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 84\u001b[0m, in \u001b[0;36mdescribe_image_with_attention\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(attention_maps):  \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m         \u001b[43mvisualize_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_maps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caption\n",
      "Cell \u001b[1;32mIn[34], line 37\u001b[0m, in \u001b[0;36mvisualize_attention\u001b[1;34m(image, attention_map, word, patch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot reshape attention of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_patches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into square grid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mreshape(h, w)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 37\u001b[0m attn \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m-\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (attn\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m attn\u001b[38;5;241m.\u001b[39mmin())  \u001b[38;5;66;03m# normalize to [0,1]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Resize attention to match image size\u001b[39;00m\n\u001b[0;32m     40\u001b[0m attn_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(attn, image\u001b[38;5;241m.\u001b[39msize)  \u001b[38;5;66;03m# use OpenCV to resize heatmap\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:45\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# describe_image_with_attention(Image.open('./glass.jpg').convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "102d3c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# --- Backward pass ---\u001b[39;00m\n\u001b[0;32m     60\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 61\u001b[0m \u001b[43mtoken_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# --- Compute CAM ---\u001b[39;00m\n\u001b[0;32m     64\u001b[0m weights \u001b[38;5;241m=\u001b[39m gradients\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# (1, 1, D)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load model ---\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "adapter_path = \"./blip-lora-finetuned/checkpoint-4770/\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "raw_image = Image.open('./glass.jpg').convert(\"RGB\")\n",
    "inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs['pixel_values']\n",
    "\n",
    "# --- Hook setup ---\n",
    "activations = None\n",
    "gradients = None\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    global activations\n",
    "    activations = output\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    global gradients\n",
    "    gradients = grad_output[0]\n",
    "\n",
    "# Register hooks on last encoder layer's attention QKV\n",
    "target_layer = model.base_model.vision_model.encoder.layers[-1].self_attn.qkv\n",
    "fwd_handle = target_layer.register_forward_hook(forward_hook)\n",
    "bwd_handle = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "# --- Forward with gradient ---\n",
    "model.train()  # Enable gradients (important!)\n",
    "outputs = model.generate(\n",
    "    pixel_values=pixel_values,\n",
    "    max_length=30,\n",
    "    num_beams=1,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "# --- Pick target token index (e.g. 3rd token) ---\n",
    "token_ids = outputs.sequences[0]\n",
    "token_index = 3\n",
    "target_token_id = token_ids[token_index]\n",
    "token_score = outputs.scores[token_index - 1][0, target_token_id]  # scores are offset by 1\n",
    "\n",
    "# --- Backward pass ---\n",
    "model.zero_grad()\n",
    "token_score.backward(retain_graph=True)\n",
    "\n",
    "# --- Compute CAM ---\n",
    "weights = gradients.mean(dim=1, keepdim=True)         # (1, 1, D)\n",
    "cam = (weights * activations).sum(dim=-1)             # (1, N)\n",
    "cam = F.relu(cam)\n",
    "cam = cam / cam.max()\n",
    "cam = cam.squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Remove CLS token (if ViT), reshape\n",
    "cam = cam[1:]  # BLIP ViT uses [CLS] at index 0\n",
    "num_patches = int(cam.shape[0] ** 0.5)\n",
    "cam = cam.reshape(num_patches, num_patches)\n",
    "cam = cv2.resize(cam, raw_image.size)  # Resize to image size\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "\n",
    "# --- Overlay and visualize ---\n",
    "superimposed_img = heatmap * 0.4 + np.array(raw_image)\n",
    "\n",
    "decoded_token = processor.tokenizer.decode([target_token_id])\n",
    "plt.imshow(superimposed_img.astype(np.uint8))\n",
    "plt.title(f\"Token: '{decoded_token}'\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Clean up\n",
    "fwd_handle.remove()\n",
    "bwd_handle.remove()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
