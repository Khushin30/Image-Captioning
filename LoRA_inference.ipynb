{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from peft import PeftModel, PeftConfig # Make sure PEFT is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\" # Use the SAME base model you trained on\n",
    "adapter_path = \"./blip-lora-finetuned/checkpoint-4770/\" # Path to your saved LoRA adapter directory\n",
    "image_path = \"./glass.jpg\" # <--- CHANGE THIS to your image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Determine Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Salesforce/blip-image-captioning-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Base Model and Processor ---\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter from: ./blip-lora-finetuned/checkpoint-4770/\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load the LoRA Adapter ---\n",
    "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
    "# This automatically loads the LoRA configuration and weights and applies them\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.to(device) # Move the combined model to the device\n",
    "model.eval() # Set the model to evaluation mode (important!)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image: ./glass.jpg\n",
      "Processing image...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Prepare Input Image ---\n",
    "try:\n",
    "    print(f\"Loading image: {image_path}\")\n",
    "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Image file not found at {image_path}\")\n",
    "    exit() # Or handle the error appropriately\n",
    "\n",
    "# Process the image (no text prompt needed for unconditional captioning)\n",
    "print(\"Processing image...\")\n",
    "inputs = processor(images=raw_image, return_tensors=\"pt\").to(device)\n",
    "pixel_values = inputs['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating caption...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Generate Caption ---\n",
    "print(\"Generating caption...\")\n",
    "# Use torch.no_grad() for inference to disable gradient calculations\n",
    "with torch.no_grad():\n",
    "    # You can adjust generation parameters (max_length, num_beams, etc.)\n",
    "    outputs = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        max_length=50,         # Maximum length of the generated caption\n",
    "        num_beams=5,           # Use beam search for potentially better results\n",
    "        early_stopping=True    # Stop generation early if EOS token is produced\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding caption...\n",
      "------------------------------\n",
      "Generated Caption: a clear glass on a white background\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Decode the Output ---\n",
    "print(\"Decoding caption...\")\n",
    "# outputs contains token IDs, decode them back to text\n",
    "# The output is a batch, so we take the first element [0]\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Generated Caption: {caption}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garbage below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 epoch training: a teddy bear sits in a pile of blocks and blocks \\\n",
    "2 epoch training: a teddy is sitting in front of colorful blocks and blocks \\\n",
    "5 epoch training: a teddy bear sits in front of colorful blocks and blocks \\\n",
    "8 epoch training: a teddy bear sits in front of colorful blocks and blocks \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine \\\n",
    "blue smart watch with blue band \\\n",
    "a black leather wallet with a card holder \\\n",
    "three figures of a cat, a dog and a cat \\\n",
    "a clear glass on a white background \\\n",
    "a piece of the heart of the universe is a piece of the heart of the universe in a piece of the universe \\\n",
    "blue plushie with purple eyes and purple eyes \\\n",
    "harry and hermi harry and hermi harry potter potter potter potter potter potter potter potter potter \\\n",
    "a carpet with a vacuum on it \\\n",
    "a box filled with lots of colorfully plush toys \\\n",
    "a blue toy with a brown nose and brown eyes \\\n",
    "\n",
    "Base\\\n",
    "a blue smart watch with a white background \\\n",
    "a black leather wallet with a credit card holder \\\n",
    "three figuris of different sizes and colors \\\n",
    "a glass with a white background \\\n",
    "925 sterling silver plated necklace with ope ope ope ope ope op \\\n",
    "a blue octopus stuffed animal with a purple and purple tail \\\n",
    "harry and hermik harry and hermik harry and hermik harry and hermi \\\n",
    "a vacuum is on the floor with a vacuum \\\n",
    "a pile of stuffed animals \\\n",
    "a blue dog toy with brown ears \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning: 100%|██████████| 14/14 [03:38<00:00, 15.58s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm  # for progress bar\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "adapter_path = \"./blip-lora-finetuned/checkpoint-4770/\"\n",
    "image_folder = \"./imgs/\"  # <-- Your folder with .jpg files\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "img_mappings = defaultdict(list)\n",
    "\n",
    "\n",
    "# --- Load Model and Processor ---\n",
    "processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "base_model = BlipForConditionalGeneration.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Captioning Function ---\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values=inputs['pixel_values'],\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# --- Loop Through Images ---\n",
    "jpg_files = [f for f in os.listdir(image_folder) if f.lower().endswith(\".jpg\")]\n",
    "print(f\"Found {len(jpg_files)} images.\")\n",
    "\n",
    "for file_name in tqdm(jpg_files, desc=\"Captioning\"):\n",
    "    full_path = os.path.join(image_folder, file_name)\n",
    "    caption = generate_caption(full_path)\n",
    "    if caption:\n",
    "        img_mappings[file_name].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:   0%|          | 0/14 [00:00<?, ?it/s]C:\\Users\\patel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Captioning:   7%|▋         | 1/14 [00:02<00:38,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a blue smart watch with a white background\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  14%|█▍        | 2/14 [00:05<00:32,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a blue dog toy with brown ears\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  21%|██▏       | 3/14 [00:07<00:27,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a glass with a white background\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  29%|██▊       | 4/14 [00:12<00:33,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry and hermik harry and hermik harry and hermik harry and hermi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  36%|███▌      | 5/14 [00:15<00:28,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a stuffed lemon with a leaf on its head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  43%|████▎     | 6/14 [00:19<00:29,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disney cars lightning mcqueen die die die die die die die die die die die die die die die\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  50%|█████     | 7/14 [00:24<00:27,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925 sterling silver plated necklace with ope ope ope ope ope op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  57%|█████▋    | 8/14 [00:27<00:22,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a blue octopus stuffed animal with a purple and purple tail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  64%|██████▍   | 9/14 [00:29<00:16,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pile of stuffed animals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  71%|███████▏  | 10/14 [00:33<00:12,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a teddy bear sitting on top of a table with toys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  79%|███████▊  | 11/14 [00:37<00:10,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new style of the emu emu emu emu emu emu emu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  86%|████████▌ | 12/14 [00:40<00:06,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a vacuum is on the floor with a vacuum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning:  93%|█████████▎| 13/14 [00:43<00:03,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black leather wallet with a credit card holder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning: 100%|██████████| 14/14 [00:46<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three figuris of different sizes and colors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(base_model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(base_model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_base_captions(image_path):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Generate caption\n",
    "    inputs = processor(image,text='',  return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    output = model.generate(**inputs)\n",
    "\n",
    "    # Print caption\n",
    "    print(processor.decode(output[0], skip_special_tokens=True))\n",
    "    return processor.decode(output[0])\n",
    "\n",
    "jpg_files = [f for f in os.listdir(image_folder) if f.lower().endswith(\".jpg\")]\n",
    "print(f\"Found {len(jpg_files)} images.\")\n",
    "\n",
    "for file_name in tqdm(jpg_files, desc=\"Captioning\"):\n",
    "    full_path = os.path.join(image_folder, file_name)\n",
    "    caption = generate_base_captions(full_path)\n",
    "    if caption:\n",
    "        img_mappings[file_name].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'dig watch.jpg': ['blue smart watch with blue band',\n",
       "              'a blue smart watch with a white background [SEP]'],\n",
       "             'dog.jpg': ['a blue toy with a brown nose and brown eyes',\n",
       "              'a blue dog toy with brown ears [SEP]'],\n",
       "             'glass.jpg': ['a clear glass on a white background',\n",
       "              'a glass with a white background [SEP]'],\n",
       "             'harry.jpg': ['harry and hermi harry and hermi harry potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter',\n",
       "              'harry and hermik harry and hermik harry and hermik harry and hermi'],\n",
       "             'lem.jpg': ['cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu',\n",
       "              'a stuffed lemon with a leaf on its head [SEP]'],\n",
       "             'mc.jpg': ['disney cars die die die die die die',\n",
       "              'disney cars lightning mcqueen die die die die die die die die die die die die die die die'],\n",
       "             'neck.jpg': ['a piece of the heart of the universe is a piece of the heart of the universe in a piece of the universe',\n",
       "              '925 sterling silver plated necklace with ope ope ope ope ope op'],\n",
       "             'oct.jpg': ['blue plushie with purple eyes and purple eyes',\n",
       "              'a blue octopus stuffed animal with a purple and purple tail [SEP]'],\n",
       "             'plush.jpg': ['a box filled with lots of colorfully plush toys',\n",
       "              'a pile of stuffed animals [SEP]'],\n",
       "             'red-car.jpg': ['a teddy bear sits in front of colorful blocks and blocks',\n",
       "              'a teddy bear sitting on top of a table with toys [SEP]'],\n",
       "             'turd.jpg': ['po po po po po po po po po po',\n",
       "              'the new style of the emu emu emu emu emu emu emu'],\n",
       "             'vac.jpg': ['a carpet with a vacuum on it',\n",
       "              'a vacuum is on the floor with a vacuum [SEP]'],\n",
       "             'wallet.jpg': ['a black leather wallet with a card holder',\n",
       "              'a black leather wallet with a credit card holder [SEP]'],\n",
       "             'who knows.jpg': ['three figures of a cat, a dog and a cat',\n",
       "              'three figuris of different sizes and colors [SEP]']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'img': 'dig watch.jpg',\n",
       "  'fineTuned': 'blue smart watch with blue band',\n",
       "  'base': 'a blue smart watch with a white background',\n",
       "  'eval': 'It improves on color-specific details like the eyes and nose but loses higher-level understanding—like recognizing it as a dog or noting the ears.'},\n",
       " {'img': 'dog.jpg',\n",
       "  'fineTuned': 'a blue toy with a brown nose and brown eyes',\n",
       "  'base': 'a blue dog toy with brown ears',\n",
       "  'eval': 'It seems to have learned how to pick out color of eyes and nose, althought it seems to forget that it is a dog and information about ears.'},\n",
       " {'img': 'glass.jpg',\n",
       "  'fineTuned': 'a clear glass on a white background',\n",
       "  'base': 'a glass with a white background',\n",
       "  'eval': 'This is one of the images it does much better on as not only does it retain all the info from the base model here but also learns what clear should look like.'},\n",
       " {'img': 'harry.jpg',\n",
       "  'fineTuned': 'harry and hermi harry and hermi harry potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter',\n",
       "  'base': 'harry and hermik harry and hermik harry and hermik harry and hermi',\n",
       "  'eval': \"Both models struggle here, falling into repetitive loops. The fine-tuned one obsessively outputs 'Potter' like the Dark Lord himself stuck in an echo chamber.\"},\n",
       " {'img': 'lem.jpg',\n",
       "  'fineTuned': 'cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu',\n",
       "  'base': 'a stuffed lemon with a leaf on its head',\n",
       "  'eval': 'We can see major degradation here because fine-tuned model collapses into a meaningless repetition loop, while the base gets it right (a stuffed lemon with a leaf).'},\n",
       " {'img': 'plush.jpg',\n",
       "  'fineTuned': 'a box filled with lots of colorfully plush toys',\n",
       "  'base': 'a pile of stuffed animals',\n",
       "  'eval': 'Keeping aside the fact it somehow thinks there is a box here, it does quite a good job learning stuffed animal=plush toy and it adds the detail that there are many different colored ones here.'},\n",
       " {'img': 'teddy.jpg',\n",
       "  'fineTuned': 'a teddy bear sits in front of colorful blocks and blocks',\n",
       "  'base': 'a teddy bear sitting on top of a table with toys',\n",
       "  'eval': 'Shows improved object context—identifying surrounding blocks and their colors—but has repetition issues and misses that the bear is sitting on a table.'},\n",
       " {'img': 'turd.jpg',\n",
       "  'fineTuned': 'po po po po po po po po po po',\n",
       "  'base': 'the new style of the emu emu emu emu emu emu emu',\n",
       "  'eval': 'Both models fail here—fine-tuned one descends into “po po po,” as if attempting to summon the Dragon Warrior himself. No real understanding is demonstrated by either model.'},\n",
       " {'img': 'vac.jpg',\n",
       "  'fineTuned': 'a carpet with a vacuum on it',\n",
       "  'base': 'a vacuum is on the floor with a vacuum',\n",
       "  'eval': 'This is a bit of an improvement as it seems to have figured out what carpet it as compared to the base model.'},\n",
       " {'img': 'wallet.jpg',\n",
       "  'fineTuned': 'a black leather wallet with a card holder',\n",
       "  'base': 'a black leather wallet with a credit card holder',\n",
       "  'eval': 'This is surprisingly better as it retains information about leather and wallet but learns that you dont have to put credit cards specifically in there it can be any card.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'dig watch.jpg': [\n",
    "        'blue smart watch with blue band',\n",
    "        'a blue smart watch with a white background',\n",
    "        'It improves on color-specific details like the eyes and nose but loses higher-level understanding—like recognizing it as a dog or noting the ears.'\n",
    "    ],\n",
    "    'dog.jpg': [\n",
    "        'a blue toy with a brown nose and brown eyes',\n",
    "        'a blue dog toy with brown ears',\n",
    "        'It seems to have learned how to pick out color of eyes and nose, althought it seems to forget that it is a dog and information about ears.'\n",
    "    ],\n",
    "    'glass.jpg': [\n",
    "        'a clear glass on a white background',\n",
    "        'a glass with a white background',\n",
    "        'This is one of the images it does much better on as not only does it retain all the info from the base model here but also learns what clear should look like.'\n",
    "    ],\n",
    "    'harry.jpg': [\n",
    "        'harry and hermi harry and hermi harry potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter potter',\n",
    "        'harry and hermik harry and hermik harry and hermik harry and hermi',\n",
    "        \"Both models struggle here, falling into repetitive loops. The fine-tuned one obsessively outputs 'Potter' like the Dark Lord himself stuck in an echo chamber.\"\n",
    "\n",
    "    ],\n",
    "    'lem.jpg': [\n",
    "        'cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu cu',\n",
    "        'a stuffed lemon with a leaf on its head',\n",
    "        \"We can see major degradation here because fine-tuned model collapses into a meaningless repetition loop, while the base gets it right (a stuffed lemon with a leaf).\"\n",
    "    ],\n",
    "    'plush.jpg': [\n",
    "        'a box filled with lots of colorfully plush toys',\n",
    "        'a pile of stuffed animals',\n",
    "        'Keeping aside the fact it somehow thinks there is a box here, it does quite a good job learning stuffed animal=plush toy and it adds the detail that there are many different colored ones here.'\n",
    "    ],\n",
    "    'teddy.jpg': [\n",
    "        'a teddy bear sits in front of colorful blocks and blocks',\n",
    "        'a teddy bear sitting on top of a table with toys',\n",
    "        \"Shows improved object context—identifying surrounding blocks and their colors—but has repetition issues and misses that the bear is sitting on a table.\"\n",
    "    ],\n",
    "    'turd.jpg': [\n",
    "        'po po po po po po po po po po',\n",
    "        'the new style of the emu emu emu emu emu emu emu',\n",
    "        'Both models fail here—fine-tuned one descends into “po po po,” as if attempting to summon the Dragon Warrior himself. No real understanding is demonstrated by either model.'\n",
    "    ],\n",
    "    'vac.jpg': [\n",
    "        'a carpet with a vacuum on it',\n",
    "        'a vacuum is on the floor with a vacuum',\n",
    "        \"This is a bit of an improvement as it seems to have figured out what carpet it as compared to the base model.\"\n",
    "    ],\n",
    "    'wallet.jpg': [\n",
    "        'a black leather wallet with a card holder',\n",
    "        'a black leather wallet with a credit card holder',\n",
    "        \"This is surprisingly better as it retains information about leather and wallet but learns that you dont have to put credit cards specifically in there it can be any card.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "result = []\n",
    "for key, arr in data.items():\n",
    "    result.append({\n",
    "        'img': key,\n",
    "        'fineTuned': arr[0],\n",
    "        'base': arr[1],\n",
    "        'eval': arr[2],\n",
    "    })\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dig watch.jpg',\n",
       " 'dog.jpg',\n",
       " 'glass.jpg',\n",
       " 'harry.jpg',\n",
       " 'lem.jpg',\n",
       " 'mc.jpg',\n",
       " 'neck.jpg',\n",
       " 'oct.jpg',\n",
       " 'plush.jpg',\n",
       " 'red-car.jpg',\n",
       " 'turd.jpg',\n",
       " 'vac.jpg',\n",
       " 'wallet.jpg',\n",
       " 'who knows.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images.\n",
      "All images resized and saved.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_folder = \"./imgs/\"\n",
    "output_folder = os.path.join(image_folder, \"resized\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "target_size = 512  # Change this to your desired square size\n",
    "\n",
    "jpg_files = [f for f in os.listdir(image_folder) if f.lower().endswith(\".jpg\")]\n",
    "print(f\"Found {len(jpg_files)} images.\")\n",
    "\n",
    "whites = ['mc','neck','oct', 'vac', 'red','wallet', 'who knows']\n",
    "\n",
    "for file_name in jpg_files:\n",
    "    img_path = os.path.join(image_folder, file_name)\n",
    "    \n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Get current dimensions\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    # Compute new dimensions preserving aspect ratio\n",
    "    ratio = min(target_size / w, target_size / h)\n",
    "    new_w, new_h = int(w * ratio), int(h * ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    # Create a square image with black background\n",
    "    square_img = np.zeros((target_size, target_size, 3), dtype=np.uint8)\n",
    "    for exception in whites:\n",
    "        if exception in file_name:\n",
    "            square_img = np.ones((target_size, target_size, 3), dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate padding to center the resized image\n",
    "    top = (target_size - new_h) // 2\n",
    "    left = (target_size - new_w) // 2\n",
    "\n",
    "    # Place resized image into the square canvas\n",
    "    square_img[top:top+new_h, left:left+new_w] = resized_img\n",
    "\n",
    "    # Save the output\n",
    "    output_path = os.path.join(output_folder, file_name)\n",
    "    cv2.imwrite(output_path, square_img)\n",
    "\n",
    "print(\"All images resized and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
